{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nearpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHRifLoHaJay",
        "outputId": "7daecfc4-4e0a-43c3-8542-c39ef5feee85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nearpy\n",
            "  Downloading NearPy-1.0.0-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting bitarray\n",
            "  Downloading bitarray-2.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
            "\u001b[K     |████████████████████████████████| 241 kB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from nearpy) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from nearpy) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from nearpy) (0.16.0)\n",
            "Installing collected packages: bitarray, nearpy\n",
            "Successfully installed bitarray-2.6.0 nearpy-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(__doc__)\n",
        "import matplotlib\n",
        "#matplotlib.use('TkAgg')\n",
        "\n",
        "import heapq\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import math\n",
        "from scipy import spatial\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "\n",
        "class FacilityLocation:\n",
        "\n",
        "    def __init__(self, D, V, alpha=1.):\n",
        "        '''\n",
        "        Args\n",
        "        - D: np.array, shape [N, N], similarity matrix\n",
        "        - V: list of int, indices of columns of D\n",
        "        - alpha: float\n",
        "        '''\n",
        "        self.D = D\n",
        "        self.curVal = 0\n",
        "        self.curMax = np.zeros(len(D))\n",
        "        self.gains = []\n",
        "        self.alpha = alpha\n",
        "        self.f_norm = self.alpha / self.f_norm(V)\n",
        "        self.norm = 1. / self.inc(V, [])\n",
        "\n",
        "    def f_norm(self, sset):\n",
        "        return self.D[:, sset].max(axis=1).sum()\n",
        "\n",
        "    def inc(self, sset, ndx):\n",
        "        if len(sset + [ndx]) > 1:\n",
        "            if not ndx:  # normalization\n",
        "                return math.log(1 + self.alpha * 1)\n",
        "            return self.norm * math.log(1 + self.f_norm * np.maximum(self.curMax, self.D[:, ndx]).sum()) - self.curVal\n",
        "        else:\n",
        "            return self.norm * math.log(1 + self.f_norm * self.D[:, ndx].sum()) - self.curVal\n",
        "\n",
        "    def add(self, sset, ndx):\n",
        "        cur_old = self.curVal\n",
        "        if len(sset + [ndx]) > 1:\n",
        "            self.curMax = np.maximum(self.curMax, self.D[:, ndx])\n",
        "        else:\n",
        "            self.curMax = self.D[:, ndx]\n",
        "        self.curVal = self.norm * math.log(1 + self.f_norm * self.curMax.sum())\n",
        "        self.gains.extend([self.curVal - cur_old])\n",
        "        return self.curVal\n",
        "\n",
        "\n",
        "def _heappush_max(heap, item):\n",
        "    heap.append(item)\n",
        "    heapq._siftdown_max(heap, 0, len(heap)-1)\n",
        "\n",
        "\n",
        "def _heappop_max(heap):\n",
        "    \"\"\"Maxheap version of a heappop.\"\"\"\n",
        "    lastelt = heap.pop()  # raises appropriate IndexError if heap is empty\n",
        "    if heap:\n",
        "        returnitem = heap[0]\n",
        "        heap[0] = lastelt\n",
        "        heapq._siftup_max(heap, 0)\n",
        "        return returnitem\n",
        "    return lastelt\n",
        "\n",
        "\n",
        "def lazy_greedy_heap(F, V, B):\n",
        "    curVal = 0\n",
        "    sset = []\n",
        "    vals = []\n",
        "\n",
        "    order = []\n",
        "    heapq._heapify_max(order)\n",
        "    [_heappush_max(order, (F.inc(sset, index), index)) for index in V]\n",
        "\n",
        "    while order and len(sset) < B:\n",
        "        el = _heappop_max(order)\n",
        "        improv = F.inc(sset, el[1])\n",
        "\n",
        "        # check for uniques elements\n",
        "        if improv >= 0:\n",
        "            if not order:\n",
        "                curVal = F.add(sset, el[1])\n",
        "                sset.append(el[1])\n",
        "                vals.append(curVal)\n",
        "            else:\n",
        "                top = _heappop_max(order)\n",
        "                if improv >= top[0]:\n",
        "                    curVal = F.add(sset, el[1])\n",
        "                    sset.append(el[1])\n",
        "                    vals.append(curVal)\n",
        "                else:\n",
        "                    _heappush_max(order, (improv, el[1]))\n",
        "                _heappush_max(order, top)\n",
        "\n",
        "    return sset, vals\n",
        "\n",
        "\n",
        "def test():\n",
        "    n = 10\n",
        "    X = np.random.rand(n, n)\n",
        "    D = X * np.transpose(X)\n",
        "    F = FacilityLocation(D, range(0, n))\n",
        "    sset = lazy_greedy(F, range(0, n), 15)\n",
        "    print(sset)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSkinBj2aAOC",
        "outputId": "6a1e4194-00c6-4e4e-ac57-c338e6eed7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Necessary Functions for CRAIG"
      ],
      "metadata": {
        "id": "Yl3jNti60J-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from nearpy import Engine\n",
        "from nearpy.distances import EuclideanDistance\n",
        "from nearpy.filters import NearestFilter\n",
        "from nearpy.hashes import RandomBinaryProjections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#from lazy_greedy import FacilityLocation, lazy_greedy_heap\n",
        "import scipy.spatial\n",
        "# from eucl_dist.cpu_dist import dist\n",
        "# from eucl_dist.gpu_dist import dist as gdist\n",
        "#import tensorflow.compat.v2 as tf\n",
        "#tf.disable_v2_behavior()\n",
        "import tensorflow\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "from itertools import repeat\n",
        "import sklearn\n",
        "# from lazy_greedy import FacilityLocation, lazy_greedy, lazy_greedy_heap\n",
        "# from set_cover import SetCover\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "SEED = 100\n",
        "EPS = 1E-8\n",
        "PLOT_NAMES = ['lr', 'data_loss', 'epoch_loss', 'test_loss']  # 'cluster_compare', 'cosine_compare', 'euclidean_compare'\n",
        "\n",
        "\n",
        "def load_dataset(dataset, dataset_dir):\n",
        "    '''\n",
        "    Args\n",
        "    - dataset: str, one of ['cifar10', 'covtype'] or filename in `data/`\n",
        "    - dataset_dir: str, path to `data` folder\n",
        "\n",
        "    Returns\n",
        "    - X: np.array, shape [N, d]\n",
        "      - exception: shape [N, 32, 32, 3] for cifar10\n",
        "    - y: np.array, shape [N]\n",
        "    '''\n",
        "    if dataset == 'cifar10':\n",
        "        path = os.path.join(dataset_dir, 'cifar10', 'cifar10.npz')\n",
        "        with np.load(path) as npz:\n",
        "            X = npz['x']  # shape [60000, 32, 32, 3], type uint8\n",
        "            y = npz['y']  # shape [60000], type uint8\n",
        "        # convert to float in (0, 1), center at mean 0\n",
        "        X = X.astype(np.float32) / 255\n",
        "        # X -= np.mean(X, axis=0)\n",
        "    elif dataset == 'cifar10_features':\n",
        "        path = os.path.join(dataset_dir, 'cifar10', 'train_features.npz')\n",
        "        with np.load(path) as npz:\n",
        "            X = npz['features']  # shape [50000, 64], type float32\n",
        "            y = npz['labels']  # shape [50000], type int64\n",
        "    elif dataset == 'cifar10_grads':\n",
        "        # labels\n",
        "        path = os.path.join(dataset_dir, 'cifar10', 'train_features.npz')\n",
        "        with np.load(path) as npz:\n",
        "            y = npz['labels']  # shape [50000], type int64\n",
        "        # feautres\n",
        "        path = os.path.join('grad_features.npy')\n",
        "        X = np.load(path)  # shape [50000, 1000], type float16\n",
        "    elif dataset == 'mnist':\n",
        "        X_train = np.vstack([mnist.train.images, mnist.validation.images])\n",
        "        y_train = np.hstack([mnist.train.labels, mnist.validation.labels])\n",
        "        X_test = mnist.test.images\n",
        "        y_test = mnist.test.labels\n",
        "        X_train = X_train.astype(np.float32) / 255\n",
        "        X_test = X_test.astype(np.float32) / 255\n",
        "        return X_train, y_train, X_test, y_test\n",
        "\n",
        "    else:\n",
        "        num, dim, name = 0, 0, ''\n",
        "        if dataset == 'covtype':\n",
        "            num, dim = 581012, 54\n",
        "            name = 'covtype.libsvm.binary.scale'\n",
        "        elif dataset == 'ijcnn1.t' or dataset == 'ijcnn1.tr':\n",
        "            num, dim = 49990 if 'tr' in dataset else 91701, 22\n",
        "            name = dataset\n",
        "        elif dataset == 'combined_scale' or dataset == 'combined_scale.t':\n",
        "            num, dim = 19705 if '.t' in dataset else 78823, 100\n",
        "            name = dataset\n",
        "\n",
        "        X = np.zeros((num, dim), dtype=np.float32)\n",
        "        y = np.zeros(num, dtype=np.int32)\n",
        "        path = os.path.join(dataset_dir, name)\n",
        "\n",
        "        with open(path, 'r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                y[i] = float(line.split()[0])\n",
        "                for e in line.split()[1:]:\n",
        "                    cur = e.split(':')\n",
        "                    X[i][int(cur[0]) - 1] = float(cur[1])\n",
        "                i += 1\n",
        "        y = np.array(y, dtype=np.int32)\n",
        "        if name in ['ijcnn1.t', 'ijcnn1.tr']:\n",
        "            y[y == -1] = 0\n",
        "        else:\n",
        "            y = y - np.ones(len(y), dtype=np.int32)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def similarity(X, metric):\n",
        "    '''Computes the similarity between each pair of examples in X.\n",
        "\n",
        "    Args\n",
        "    - X: np.array, shape [N, d]\n",
        "    - metric: str, one of ['cosine', 'euclidean']\n",
        "\n",
        "    Returns\n",
        "    - S: np.array, shape [N, N]\n",
        "    '''\n",
        "    # print(f'Computing similarity for {metric}...', flush=True)\n",
        "    start = time.time()\n",
        "    dists = sklearn.metrics.pairwise_distances(X, metric=metric, n_jobs=1)\n",
        "    # dists = gdist(X, X, optimize_level=0, output='cpu')\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    if metric == 'cosine':\n",
        "        S = 1 - dists\n",
        "    elif metric == 'euclidean' or metric == 'l1':\n",
        "        m = np.max(dists)\n",
        "        S = m - dists\n",
        "    else:\n",
        "        raise ValueError(f'unknown metric: {metric}')\n",
        "\n",
        "    return S, elapsed\n",
        "\n",
        "\n",
        "def greedy_merge(X, y, B, part_num, metric, smtk=0, stoch_greedy=False):\n",
        "    N = len(X)\n",
        "    indices = list(range(N))\n",
        "    # np.random.shuffle(indices)\n",
        "    part_size = int(np.ceil(N / part_num))\n",
        "    part_indices = [indices[slice(i * part_size, min((i + 1) * part_size, N))] for i in range(part_num)]\n",
        "    print(f'GreeDi with {part_num} parts, finding {B} elements...', flush=True)\n",
        "\n",
        "    # pool = ThreadPool(part_num)\n",
        "    # order_mg_all, cluster_sizes_all, _, _, ordering_time, similarity_time = zip(*pool.map(\n",
        "    #     lambda p: get_orders_and_weights(\n",
        "    #         int(B / 2), X[part_indices[p], :], metric, p + 1, stoch_greedy, y[part_indices[p]]), np.arange(part_num)))\n",
        "    # pool.terminate()\n",
        "\n",
        "    order_mg_all, cluster_sizes_all, _, _, ordering_time, similarity_time, F_val = zip(*map(\n",
        "        lambda p: get_orders_and_weights(\n",
        "            int(B / 2), X[part_indices[p], :], metric, p + 1, stoch_greedy, y[part_indices[p]]), np.arange(part_num)))\n",
        "\n",
        "    # Returns the number of objects it has collected and deallocated\n",
        "    collected = gc.collect()\n",
        "    print(f'Garbage collector: collected {collected}')\n",
        "\n",
        "    # order_mg_all = np.zeros((part_num, B))\n",
        "    # cluster_sizes_all = np.zeros((part_num, B))\n",
        "    # ordering_time = np.zeros(part_num)\n",
        "    # similarity_time = np.zeros(part_num)\n",
        "    # for p in range(part_num):\n",
        "    #    order_mg_all[p,:], cluster_sizes_all[p,:], _, _, ordering_time[p], similarity_time[p] = get_orders_and_weights(\n",
        "    #         B, X[part_indices[p], :], metric, p, stoch_greedy, y[part_indices[p]])\n",
        "    order_mg_all = np.array(order_mg_all, dtype=np.int32)\n",
        "    cluster_sizes_all = np.array(cluster_sizes_all, dtype=np.float32)  # / part_num (not needed)\n",
        "    order_mg = order_mg_all.flatten(order='F')\n",
        "    weights_mg = cluster_sizes_all.flatten(order='F')\n",
        "    print(f'GreeDi stage 1: found {len(order_mg)} elements in: {np.max(ordering_time)} sec', flush=True)\n",
        "\n",
        "    # order_mg, weights_mg, order_sz, weights_sz, ordering_time, similarity_time\n",
        "    order, weights, order_sz, weights_sz, ordering_time_merge, similarity_time_merge = get_orders_and_weights(\n",
        "        B, X[order_mg, :], metric, smtk, 0, stoch_greedy, y[order_mg], weights_mg)\n",
        "    print(weights)\n",
        "    total_ordering_time = np.max(ordering_time) + ordering_time_merge\n",
        "    total_similarity_time = np.max(similarity_time) + similarity_time_merge\n",
        "    print(f'GreeDi stage 2: found {len(order)} elements in: {total_ordering_time + total_similarity_time} sec',\n",
        "          flush=True)\n",
        "    vals = order, weights, order_sz, weights_sz, total_ordering_time, total_similarity_time\n",
        "    return vals\n",
        "\n",
        "\n",
        "def greedi(X, y, B, part_num, metric, smtk=0, stoch_greedy=False, seed=-1):\n",
        "    N = len(X)\n",
        "    indices = list(range(N))\n",
        "    if seed != -1:\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(indices)  # Note: random shuffling\n",
        "    part_size = int(np.ceil(N / part_num))\n",
        "    part_indices = [indices[slice(i * part_size, min((i + 1) * part_size, N))] for i in range(part_num)]\n",
        "    print(f'GreeDi with {part_num} parts, finding {B} elements...', flush=True)\n",
        "\n",
        "    # pool = ThreadPool(part_num)\n",
        "    # order_mg_all, cluster_sizes_all, _, _, ordering_time, similarity_time = zip(*pool.map(\n",
        "    #     lambda p: get_orders_and_weights(\n",
        "    #         B, X[part_indices[p], :], metric, p + 1, stoch_greedy, y[part_indices[p]]), np.arange(part_num)))\n",
        "    # pool.terminate()\n",
        "    # Returns the number of objects it has collected and deallocated\n",
        "    # collected = gc.collect()\n",
        "    # print(f'Garbage collector: collected {collected}')\n",
        "    order_mg_all, cluster_sizes_all, _, _, ordering_time, similarity_time = zip(*map(\n",
        "        lambda p: get_orders_and_weights(\n",
        "            B, X[part_indices[p], :], metric, p + 1, stoch_greedy, y[part_indices[p]]), np.arange(part_num)))\n",
        "    gc.collect()\n",
        "\n",
        "    order_mg_all = np.array(order_mg_all, dtype=np.int32)\n",
        "    for c in np.arange(part_num):\n",
        "        order_mg_all[c] = np.array(part_indices[c])[order_mg_all[c]]\n",
        "    # order_mg_all = np.zeros((part_num, B))\n",
        "    # cluster_sizes_all = np.zeros((part_num, B))\n",
        "    # ordering_time = np.zeros(part_num)\n",
        "    # similarity_time = np.zeros(part_num)\n",
        "    # for p in range(part_num):\n",
        "    #    order_mg_all[p,:], cluster_sizes_all[p,:], _, _, ordering_time[p], similarity_time[p] = get_orders_and_weights(\n",
        "    #         B, X[part_indices[p], :], metric, p, stoch_greedy, y[part_indices[p]])\n",
        "    cluster_sizes_all = np.array(cluster_sizes_all, dtype=np.float32)  # / part_num (not needed)\n",
        "    order_mg = order_mg_all.flatten(order='F')\n",
        "    weights_mg = cluster_sizes_all.flatten(order='F')\n",
        "    print(f'GreeDi stage 1: found {len(order_mg)} elements in: {np.max(ordering_time)} sec', flush=True)\n",
        "\n",
        "    # order_mg, weights_mg, order_sz, weights_sz, ordering_time, similarity_time\n",
        "    order, weights, order_sz, weights_sz, ordering_time_merge, similarity_time_merge = get_orders_and_weights(\n",
        "        B, X[order_mg,:], metric, smtk, 0, stoch_greedy, y[order_mg], weights_mg)\n",
        "    print(weights)\n",
        "    order = order_mg[order]\n",
        "    total_ordering_time = np.max(ordering_time) + ordering_time_merge\n",
        "    total_similarity_time = np.max(similarity_time) + similarity_time_merge\n",
        "    print(f'GreeDi stage 2: found {len(order)} elements in: {total_ordering_time + total_similarity_time} sec', flush=True)\n",
        "    vals = order, weights, order_sz, weights_sz, total_ordering_time, total_similarity_time\n",
        "    return vals\n",
        "\n",
        "\n",
        "def get_facility_location_submodular_order(S, B, c, smtk=0, no=0, stoch_greedy=0, weights=None):\n",
        "    '''\n",
        "    Args\n",
        "    - S: np.array, shape [N, N], similarity matrix\n",
        "    - B: int, number of points to select\n",
        "\n",
        "    Returns\n",
        "    - order: np.array, shape [B], order of points selected by facility location\n",
        "    - sz: np.array, shape [B], type int64, size of cluster associated with each selected point\n",
        "    '''\n",
        "    # print('Computing facility location submodular order...')\n",
        "    N = S.shape[0]\n",
        "    no = smtk if no == 0 else no\n",
        "\n",
        "    if smtk > 0:\n",
        "        print(f'Calculating ordering with SMTK... part size: {len(S)}, B: {B}', flush=True)\n",
        "        np.save(f'/tmp/{no}/{smtk}-{c}', S)\n",
        "        if stoch_greedy > 0:\n",
        "            p = subprocess.check_output(\n",
        "                f'/tmp/{no}/smtk-master{smtk}/build/smraiz -sumsize {B} \\\n",
        "                 -stochastic-greedy -sg-epsilon {stoch_greedy} -flnpy /tmp/{no}/{smtk}-{c}.'\n",
        "                f'npy -pnpv -porder -ptime'.split())\n",
        "        else:\n",
        "            p = subprocess.check_output(\n",
        "                f'/tmp/{no}/smtk-master{smtk}/build/smraiz -sumsize {B} \\\n",
        "                             -flnpy /tmp/{no}/{smtk}-{c}.npy -pnpv -porder -ptime'.split())\n",
        "        s = p.decode(\"utf-8\")\n",
        "        str, end = ['([', ',])']\n",
        "        order = s[s.find(str) + len(str):s.rfind(end)].split(',')\n",
        "        greedy_time = float(s[s.find('CPU') + 4 : s.find('s (User')])\n",
        "        str = 'f(Solution) = '\n",
        "        F_val = float(s[s.find(str) + len(str) : s.find('Summary Solution') - 1])\n",
        "    else:\n",
        "        V = list(range(N))\n",
        "        start = time.time()\n",
        "        F = FacilityLocation(S, V)\n",
        "        order, _ = lazy_greedy_heap(F, V, B)\n",
        "        greedy_time = time.time() - start\n",
        "        F_val = 0\n",
        "\n",
        "    order = np.asarray(order, dtype=np.int64)\n",
        "    sz = np.zeros(B, dtype=np.float64)\n",
        "    for i in range(N):\n",
        "        if weights is None:\n",
        "            sz[np.argmax(S[i, order])] += 1\n",
        "        else:\n",
        "            sz[np.argmax(S[i, order])] += weights[i]\n",
        "    # print('time (sec) for computing facility location:', greedy_time, flush=True)\n",
        "    collected = gc.collect()\n",
        "    return order, sz, greedy_time, F_val\n",
        "\n",
        "\n",
        "def faciliy_location_order(c, X, y, metric, num_per_class, smtk, no, stoch_greedy, weights=None):\n",
        "    class_indices = np.where(y == c)[0]\n",
        "    print(c)\n",
        "    print(class_indices)\n",
        "    print(len(class_indices))\n",
        "    S, S_time = similarity(X[class_indices], metric=metric)\n",
        "    order, cluster_sz, greedy_time, F_val = get_facility_location_submodular_order(\n",
        "        S, num_per_class, c, smtk, no, stoch_greedy, weights)\n",
        "    return class_indices[order], cluster_sz, greedy_time, S_time\n",
        "\n",
        "\n",
        "def save_all_orders_and_weights(folder, X, metric='l2', stoch_greedy=False, y=None, equal_num=False, outdir='.'):\n",
        "    N = X.shape[0]\n",
        "    if y is None:\n",
        "        y = np.zeros(N, dtype=np.int32)  # assign every point to the same class\n",
        "    classes = np.unique(y)\n",
        "    C = len(classes)  # number of classes\n",
        "    # assert np.array_equal(classes, np.arange(C))\n",
        "    # assert B % C == 0\n",
        "    class_nums = [sum(y == c) for c in classes]\n",
        "    print(class_nums)\n",
        "    class_indices = [np.where(y == c)[0] for c in classes]\n",
        "\n",
        "    tmp_path = '/tmp'\n",
        "    no, smtk = 2, 2\n",
        "\n",
        "    def greedy(B, c):\n",
        "        print('Computing facility location submodular order...')\n",
        "        print(f'Calculating ordering with SMTK... part size: {class_nums[c]}, B: {B}', flush=True)\n",
        "        command = f'/tmp/{no}/smtk-master{smtk}/build/smraiz -sumsize {B} \\\n",
        "                                 -flnpy {tmp_path}/{no}/{smtk}-{c}.npy -pnpv -porder -ptime'\n",
        "        if stoch_greedy:\n",
        "            command += f' -stochastic-greedy -sg-epsilon {.9}'\n",
        "\n",
        "        p = subprocess.check_output(command.split())\n",
        "        s = p.decode(\"utf-8\")\n",
        "        str, end = ['([', ',])']\n",
        "        order = s[s.find(str) + len(str):s.rfind(end)].split(',')\n",
        "        order = np.asarray(order, dtype=np.int64)\n",
        "        greedy_time = float(s[s.find('CPU') + 4: s.find('s (User')])\n",
        "        print(f'FL greedy time: {greedy_time}', flush=True)\n",
        "        str = 'f(Solution) = '\n",
        "        F_val = float(s[s.find(str) + len(str) : s.find('Summary Solution') - 1])\n",
        "        print(f'===========> f(Solution) = {F_val}')\n",
        "        print('time (sec) for computing facility location:', greedy_time, flush=True)\n",
        "        return order, greedy_time, F_val\n",
        "\n",
        "    def get_subset_sizes(B, equal_num):\n",
        "        if equal_num:\n",
        "            # class_nums = [sum(y == c) for c in classes]\n",
        "            num_per_class = int(np.ceil(B / C)) * np.ones(len(classes), dtype=np.int32)\n",
        "            minority = class_nums < np.ceil(B / C)\n",
        "            if sum(minority) > 0:\n",
        "                extra = sum([max(0, np.ceil(B / C) - class_nums[c]) for c in classes])\n",
        "                for c in classes[~minority]:\n",
        "                    num_per_class[c] += int(np.ceil(extra / sum(minority)))\n",
        "        else:\n",
        "            num_per_class = np.int32(np.ceil(np.divide([sum(y == i) for i in classes], N) * B))\n",
        "\n",
        "        return num_per_class\n",
        "\n",
        "    def merge_orders(order_mg_all, weights_mg_all, equal_num):\n",
        "        order_mg, weights_mg = [], []\n",
        "        if equal_num:\n",
        "            props = np.rint([len(order_mg_all[i]) for i in range(len(order_mg_all))])\n",
        "        else:\n",
        "            # merging imbalanced classes\n",
        "            class_ratios = np.divide([np.sum(y == i) for i in classes], N)\n",
        "            props = np.rint(class_ratios / np.min(class_ratios))\n",
        "            print(f'Selecting with ratios {np.array(class_ratios)}')\n",
        "            print(f'Class proportions {np.array(props)}')\n",
        "\n",
        "        order_mg_all = np.array(order_mg_all)\n",
        "        weights_mg_all = np.array(weights_mg_all)\n",
        "        for i in range(int(np.rint(np.max([len(order_mg_all[c]) / props[c] for c in classes])))):\n",
        "            for c in classes:\n",
        "                ndx = slice(i * int(props[c]), int(min(len(order_mg_all[c]), (i + 1) * props[c])))\n",
        "                order_mg = np.append(order_mg, order_mg_all[c][ndx])\n",
        "                weights_mg = np.append(weights_mg, weights_mg_all[c][ndx])\n",
        "        order_mg = np.array(order_mg, dtype=np.int32)\n",
        "        weights_mg = np.array(weights_mg, dtype=np.float)\n",
        "        return order_mg, weights_mg\n",
        "\n",
        "    def calculate_weights(order, c):\n",
        "        weight = np.zeros(len(order), dtype=np.float64)\n",
        "        center = np.argmax(D[str(c)][:, order], axis=1)\n",
        "        for i in range(len(order)):\n",
        "            weight[i] = np.sum(center == i)\n",
        "        return weight\n",
        "\n",
        "    D, m = {}, 0\n",
        "    similarity_times, max_similarity = [], []\n",
        "    for c in classes:\n",
        "        print(f'Computing distances for class {c}...')\n",
        "        time.sleep(.1)\n",
        "        start = time.time()\n",
        "        if metric in ['', 'l2', 'l1']:\n",
        "            dists = sklearn.metrics.pairwise_distances(X[class_indices[c]], metric=metric, n_jobs=1)\n",
        "        else:\n",
        "            p = float(metric)\n",
        "            dim = class_nums[c]\n",
        "            dists = np.zeros((dim, dim))\n",
        "            for i in range(dim):\n",
        "                dists[i,:] = np.power(np.sum(np.power(np.abs(X[class_indices[c][i]] - X[class_indices[c]]), p), axis=1), 1./p)\n",
        "                # for j in range(i+1, dim):\n",
        "                #     dists[i,j] = np.power(np.sum(np.power(np.abs(X[class_indices[c][i]] - X[class_indices[c][j]]), p)), 1./p)\n",
        "            # dists[np.triu_indices(dim, 1)] = d\n",
        "            # dists = dists.T + dists\n",
        "        similarity_times.append(time.time() - start)\n",
        "        print(f'similarity times: {similarity_times}')\n",
        "        print('Computing max')\n",
        "        m = np.max(dists)\n",
        "        print(f'max: {m}')\n",
        "        S = m - dists\n",
        "        np.save(f'{tmp_path}/{no}/{smtk}-{c}', S)\n",
        "        D[str(c)] = S\n",
        "        max_similarity.append(m)\n",
        "\n",
        "    # Ordering all the data with greedy\n",
        "    print(f'Greedy: selecting {class_nums} elements')\n",
        "    # order_in_class, greedy_times, F_vals = zip(*map(lambda c: greedy(class_nums[c], c), classes))\n",
        "    # order_all = [class_indices[c][order_in_class[c]] for c in classes]\n",
        "\n",
        "    for subset_size in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
        "    # for subset_size in [0.9, 1.0]:\n",
        "        B = int(N * subset_size)\n",
        "        num_per_class = get_subset_sizes(B, equal_num)\n",
        "\n",
        "        # Note: for marginal gains\n",
        "        order_in_class, greedy_times, F_vals = zip(*map(lambda c: greedy(num_per_class[c], c), classes))\n",
        "        order_all = [class_indices[c][order_in_class[c]] for c in classes]\n",
        "        #####\n",
        "\n",
        "        weights = [calculate_weights(order_in_class[c][:num_per_class[c]], c) for c in classes]\n",
        "        order_subset = [order_all[c][:num_per_class[c]] for c in classes]\n",
        "        order_merge, weights_merge = merge_orders(order_subset, weights, equal_num)\n",
        "        F_vals = np.divide(F_vals, class_nums)\n",
        "\n",
        "        folder = '/tmp/covtype'\n",
        "        print(f'saving to {folder}_{subset_size}_{metric}_w.npz')\n",
        "        np.savez(f'{folder}_{subset_size}_{metric}_w', order=order_merge, weight=weights_merge,\n",
        "                 order_time=greedy_times, similarity_time=similarity_times, F_vals=F_vals, max_dist=m)\n",
        "    # end for on subset sizes\n",
        "    # return vals\n",
        "\n",
        "\n",
        "def get_orders_and_weights(B, X, metric, smtk, no=0, stoch_greedy=0, y=None, weights=None, equal_num=False, outdir='.'):\n",
        "    '''\n",
        "    Ags\n",
        "    - X: np.array, shape [N, d]\n",
        "    - B: int, number of points to select\n",
        "    - metric: str, one of ['cosine', 'euclidean'], for similarity\n",
        "    - y: np.array, shape [N], integer class labels for C classes\n",
        "      - if given, chooses B / C points per class, B must be divisible by C\n",
        "    - outdir: str, path to output directory, must already exist\n",
        "\n",
        "    Returns\n",
        "    - order_mg/_sz: np.array, shape [B], type int64\n",
        "      - *_mg: order points by their marginal gain in FL objective (largest gain first)\n",
        "      - *_sz: order points by their cluster size (largest size first)\n",
        "    - weights_mg/_sz: np.array, shape [B], type float32, sums to 1\n",
        "    '''\n",
        "    N = X.shape[0]\n",
        "    if y is None:\n",
        "        y = np.zeros(N, dtype=np.int32)  # assign every point to the same class\n",
        "    classes = np.unique(y)\n",
        "    C = len(classes)  # number of classes\n",
        "    # assert np.array_equal(classes, np.arange(C))\n",
        "    # assert B % C == 0\n",
        "\n",
        "    if equal_num:\n",
        "        class_nums = [sum(y == c) for c in classes]\n",
        "        num_per_class = int(np.ceil(B / C)) * np.ones(len(classes), dtype=np.int32)\n",
        "        minority = class_nums < np.ceil(B / C)\n",
        "        if sum(minority) > 0:\n",
        "            extra = sum([max(0, np.ceil(B / C) - class_nums[c]) for c in classes])\n",
        "            for c in classes[~minority]:\n",
        "                num_per_class[c] += int(np.ceil(extra / sum(minority)))\n",
        "    else:\n",
        "        num_per_class = np.int32(np.ceil(np.divide([sum(y == i) for i in classes], N) * B))\n",
        "        print('not equal_num')\n",
        "\n",
        "    # print(f'Greedy: selecting {num_per_class} elements')\n",
        "\n",
        "    # order_mg_all = np.zeros([C, num_per_class], dtype=np.int64)\n",
        "    # cluster_sizes_all = np.zeros([C, num_per_class], dtype=np.float32)\n",
        "    # greedy_time_all = np.zeros([C, num_per_class], dtype=np.int64)\n",
        "    # similarity_time_all = np.zeros([C, num_per_class], dtype=np.int64)\n",
        "\n",
        "    # pool = ThreadPool(C)\n",
        "    # order_mg_all, cluster_sizes_all, greedy_times, similarity_times = zip(*pool.map(\n",
        "    #     lambda c: faciliy_location_order(c, X, y, metric, num_per_class[c], smtk, stoch_greedy, weights), classes))\n",
        "    # pool.terminate()\n",
        "    order_mg_all, cluster_sizes_all, greedy_times, similarity_times = zip(*map(\n",
        "        lambda c: faciliy_location_order(c, X, y, metric, num_per_class[c], smtk, no, stoch_greedy, weights), classes))\n",
        "\n",
        "    order_mg, weights_mg = [], []\n",
        "    if equal_num:\n",
        "        props = np.rint([len(order_mg_all[i]) for i in range(len(order_mg_all))])\n",
        "    else:\n",
        "        # merging imbalanced classes\n",
        "        class_ratios = np.divide([np.sum(y == i) for i in classes], N)\n",
        "        props = np.rint(class_ratios / np.min(class_ratios))\n",
        "        print(f'Selecting with ratios {np.array(class_ratios)}')\n",
        "        print(f'Class proportions {np.array(props)}')\n",
        "\n",
        "    order_mg_all = np.array(order_mg_all)\n",
        "    cluster_sizes_all = np.array(cluster_sizes_all)\n",
        "    for i in range(int(np.rint(np.max([len(order_mg_all[c]) / props[c] for c in classes])))):\n",
        "        for c in classes:\n",
        "            ndx = slice(i * int(props[c]), int(min(len(order_mg_all[c]), (i + 1) * props[c])))\n",
        "            order_mg = np.append(order_mg, order_mg_all[c][ndx])\n",
        "            weights_mg = np.append(weights_mg, cluster_sizes_all[c][ndx])\n",
        "    order_mg = np.array(order_mg, dtype=np.int32)\n",
        "\n",
        "    # class_ratios = np.divide([np.sum(y == i) for i in classes], N)\n",
        "    # weights_mg[y[order_mg] == np.argmax(class_ratios)] /= (np.max(class_ratios) / np.min(class_ratios))\n",
        "\n",
        "    weights_mg = np.array(weights_mg, dtype=np.float32)\n",
        "    ordering_time = np.max(greedy_times)\n",
        "    similarity_time = np.max(similarity_times)\n",
        "\n",
        "    # for c in classes:\n",
        "    #     class_indices = np.where(y == c)[0]\n",
        "    #     S, similarity_time_all[c] = similarity(X[class_indices], metric=metric)\n",
        "    #     order, cluster_sz, greedy_time_all[c], F_val = get_facility_location_submodular_order(S, num_per_class, c, smtk)\n",
        "    #     order_mg_all[c] = class_indices[order]\n",
        "    #     cluster_sizes_all[c] = cluster_sz\n",
        "    #     save_cluster_sizes(cluster_sizes_all[c], metric=f'{metric}_class{c}', outdir=outdir)\n",
        "    # cluster_sizes_all /= N\n",
        "\n",
        "    # choose 1st from each class, then 2nd from each class, etc.\n",
        "    # i.e. column-major order\n",
        "    # order_mg_all = np.array(order_mg_all)\n",
        "    # cluster_sizes_all = np.array(cluster_sizes_all, dtype=np.float32) / N\n",
        "    # order_mg = order_mg_all.flatten(order='F')\n",
        "    # weights_mg = cluster_sizes_all.flatten(order='F')\n",
        "\n",
        "    # sort by descending cluster size within each class\n",
        "    # cluster_order = np.argsort(-cluster_sizes_all, axis=1)\n",
        "    # rows_selector = np.arange(C)[:, np.newaxis]\n",
        "    order_sz = []  # order_mg_all[rows_selector, cluster_order].flatten(order='F')\n",
        "    weights_sz = [] # cluster_sizes_all[rows_selector, cluster_order].flatten(order='F')\n",
        "    vals = order_mg, weights_mg, order_sz, weights_sz, ordering_time, similarity_time\n",
        "    return vals"
      ],
      "metadata": {
        "id": "8Z6H4rK_aC_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Forgetting Statistics"
      ],
      "metadata": {
        "id": "4c19rIbk0Eik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_forgetting_statistics(scores, percentage = 5):\n",
        "\n",
        "    # Forgetting event is a transition in accuracy from 1 to 0\n",
        "    fgt_events = scores[:, 1:] - scores[:, :-1]\n",
        "\n",
        "    # Count of forgetting events for each point\n",
        "    fgt_ncounts = np.sum(fgt_events==-1, axis=1)\n",
        "\n",
        "    # Indices of points to be removed based on percentage\n",
        "    fgt_indices = np.argsort(fgt_ncounts)[:int(np.floor(scores.shape[0] * (float(percentage)/100)))]\n",
        "    \n",
        "    return fgt_indices"
      ],
      "metadata": {
        "id": "GONfjl8KdSUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running F-CRAIG"
      ],
      "metadata": {
        "id": "1PNDGFXB0XGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.regularizers import l2\n",
        "import sklearn.metrics\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "total_start = time.time()\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "\n",
        "num_classes, smtk = 10, 0\n",
        "Y_train_nocat = Y_train\n",
        "Y_train = np_utils.to_categorical(Y_train, num_classes)\n",
        "Y_test = np_utils.to_categorical(Y_test, num_classes)\n",
        "\n",
        "batch_size = 32\n",
        "# subset, random = False, False  # all\n",
        "subset, random = True, False  # greedy\n",
        "# subset, random = True, True  # random\n",
        "subset_size = .4 if subset else 1.0\n",
        "epochs = 10\n",
        "reg = 1e-4\n",
        "runs = 3\n",
        "save_subset = False\n",
        "use_forgettability_score = True\n",
        "\n",
        "folder = f'/content/'\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_dim=784, kernel_regularizer=l2(reg)))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(10, kernel_regularizer=l2(reg)))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='sgd')\n",
        "\n",
        "\n",
        "train_loss, test_loss = np.zeros((runs, epochs)), np.zeros((runs, epochs))\n",
        "train_acc, test_acc = np.zeros((runs, epochs)), np.zeros((runs, epochs))\n",
        "train_time = np.zeros((runs, epochs))\n",
        "fgt_score_time = np.zeros((runs, epochs))\n",
        "epoch_time, craig_time = np.zeros((runs, epochs)), np.zeros((runs, epochs))\n",
        "grd_time, sim_time, pred_time, gow_time = np.zeros((runs, epochs)), np.zeros((runs, epochs)), np.zeros((runs, epochs)), np.zeros((runs, epochs))\n",
        "not_selected = np.zeros((runs, epochs))\n",
        "times_selected = np.zeros((runs, len(X_train)))\n",
        "best_acc = 0\n",
        "print(f'----------- smtk: {smtk} ------------')\n",
        "\n",
        "if save_subset:\n",
        "    B = int(subset_size * len(X_train))\n",
        "    selected_ndx = np.zeros((runs, epochs, B))\n",
        "    selected_wgt = np.zeros((runs, epochs, B))\n",
        "\n",
        "for run in range(runs):\n",
        "    X_subset = X_train\n",
        "    Y_subset = Y_train\n",
        "    W_subset = np.ones(len(X_subset))\n",
        "    ordering_time,similarity_time, pre_time = 0, 0, 0\n",
        "    loss_vec, acc_vec, time_vec = [], [], []\n",
        "    \n",
        "    fgt_scores = np.empty([len(X_train), 0])\n",
        "    keep_indices = list(range(len(X_train)))\n",
        "    \n",
        "    for epoch in range(0, epochs):\n",
        "        start_e = time.time()\n",
        "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
        "        num_batches = int(np.ceil(X_subset.shape[0] / float(batch_size)))\n",
        "\n",
        "        for index in range(num_batches):\n",
        "            X_batch = X_subset[index * batch_size:(index + 1) * batch_size]\n",
        "            Y_batch = Y_subset[index * batch_size:(index + 1) * batch_size]\n",
        "            W_batch = W_subset[index * batch_size:(index + 1) * batch_size]\n",
        "\n",
        "            start = time.time()\n",
        "            history = model.train_on_batch(X_batch, Y_batch, sample_weight=W_batch)\n",
        "            train_time[run][epoch] += time.time() - start\n",
        "\n",
        "            start = time.time()\n",
        "            if (index % 100)==0 and use_forgettability_score:\n",
        "                Y_pred = model.predict(X_train[keep_indices])\n",
        "                Y_pred_class = np.argmax(Y_pred, axis=1)\n",
        "                batch_acc = (Y_train_nocat[keep_indices]==Y_pred_class).astype(int)\n",
        "                fgt_scores = np.concatenate([fgt_scores, np.expand_dims(batch_acc, axis=1)], axis=1)\n",
        "            fgt_score_time[run][epoch]+=time.time() - start\n",
        "        \n",
        "        if use_forgettability_score and epoch>0:\n",
        "\n",
        "            start = time.time()\n",
        "            fgt_indices = compute_forgetting_statistics(fgt_scores, percentage=5)\n",
        "            keep_indices = [keep_indices[i] for i in range(len(keep_indices)) if i not in fgt_indices]\n",
        "\n",
        "            # Update forgetting scores\n",
        "            fgt_scores = fgt_scores[[i for i in range(len(fgt_scores)) if i not in fgt_indices]]\n",
        "            fgt_score_time[run][epoch]+=time.time() - start\n",
        "\n",
        "        start_c = time.time()\n",
        "        if subset:\n",
        "            if random:\n",
        "                # indices = np.random.randint(0, len(X_train), int(subset_size * len(X_train)))\n",
        "                indices = copy.deepcopy(keep_indices)\n",
        "                np.random.shuffle(indices)\n",
        "                indices = indices[:int(subset_size * len(X_train))]\n",
        "                W_subset = np.ones(len(indices))\n",
        "            else:\n",
        "                start = time.time()\n",
        "                _logits = model.predict(X_train[keep_indices])\n",
        "                pre_time = time.time() - start\n",
        "                features = _logits - Y_train[keep_indices]\n",
        "                start = time.time()\n",
        "                indices, W_subset, _, _, ordering_time, similarity_time = get_orders_and_weights(\n",
        "                    int(subset_size * len(X_train)), features, 'euclidean', smtk, 0, False, Y_train_nocat[keep_indices])\n",
        "                indices = [keep_indices[i] for i in indices]\n",
        "                gow_time[run, epoch] = time.time() - start\n",
        "\n",
        "                W_subset = W_subset / np.sum(W_subset) * len(W_subset)  # todo\n",
        "\n",
        "            if save_subset:\n",
        "                selected_ndx[run, epoch], selected_wgt[run, epoch] = indices, W_subset\n",
        "\n",
        "            grd_time[run, epoch], sim_time[run, epoch], pred_time[run, epoch]  = ordering_time, similarity_time, pre_time\n",
        "            times_selected[run][indices] += 1\n",
        "            not_selected[run, epoch] = np.sum(times_selected[run] == 0) / len(times_selected[run]) * 100\n",
        "        else:\n",
        "            pred_time = 0\n",
        "            indices = np.arange(len(X_train))\n",
        "\n",
        "        craig_time[run][epoch] = time.time() - start_c\n",
        "        X_subset = X_train[indices, :]\n",
        "        Y_subset = Y_train[indices]\n",
        "\n",
        "        start = time.time()\n",
        "        score = model.evaluate(X_test, Y_test, verbose=1)\n",
        "        eval_time = time.time()-start\n",
        "\n",
        "        start = time.time()\n",
        "        score_loss = model.evaluate(X_train[keep_indices], Y_train[keep_indices], verbose=1)\n",
        "        print(f'eval time on training: {time.time()-start}')\n",
        "\n",
        "        test_loss[run][epoch], test_acc[run][epoch] = score[0], score[1]\n",
        "        train_loss[run][epoch], train_acc[run][epoch] = score_loss[0], score_loss[1]\n",
        "        best_acc = max(test_acc[run][epoch], best_acc)\n",
        "\n",
        "        grd = 'random_wor' if random else 'grd_normw'\n",
        "        print(f'run: {run}, {grd}, subset_size: {subset_size}, epoch: {epoch}, test_acc: {test_acc[run][epoch]}, '\n",
        "              f'loss: {train_loss[run][epoch]}, best_prec1_gb: {best_acc}, not selected %:{not_selected[run][epoch]}')\n",
        "        \n",
        "        epoch_time[run][epoch] = time.time() - start_e\n",
        "    if save_subset:\n",
        "        print(\n",
        "            f'Saving the results to {folder}_{subset_size}_{grd}_{runs}_fcraig')\n",
        "\n",
        "        np.savez(f'{folder}_{subset_size}_{grd}_{runs}_fcraig',\n",
        "                 # f'_{grd}_{args.lr_schedule}_start_{args.start_subset}_lag_{args.lag}_subset',\n",
        "                 train_loss=train_loss, test_acc=test_acc, train_acc=train_acc, test_loss=test_loss,\n",
        "                 train_time=train_time, grd_time=grd_time, sim_time=sim_time, pred_time=pred_time,\n",
        "                 not_selected=not_selected, times_selected=times_selected,\n",
        "                 subset=selected_ndx, weights=selected_wgt,\n",
        "                 fgt_score_time=fgt_score_time, gow_time = gow_time,\n",
        "                 epoch_time=epoch_time, craig_time=craig_time)\n",
        "    else:\n",
        "        print(\n",
        "            f'Saving the results to {folder}_{subset_size}_{grd}_{runs}_fcraig')\n",
        "\n",
        "        np.savez(f'{folder}_{subset_size}_{grd}_{runs}_fcraig',\n",
        "                 # f'_{grd}_{args.lr_schedule}_start_{args.start_subset}_lag_{args.lag}',\n",
        "                 train_loss=train_loss, test_acc=test_acc, train_acc=train_acc, test_loss=test_loss,\n",
        "                 train_time=train_time, grd_time=grd_time, sim_time=sim_time, pred_time=pred_time,\n",
        "                 not_selected=not_selected, times_selected=times_selected,\n",
        "                 fgt_score_time=fgt_score_time, gow_time = gow_time,\n",
        "                 epoch_time=epoch_time, craig_time=craig_time)\n",
        "        \n",
        "total_time = time.time() - total_start\n",
        "print(total_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y5VdZDgKFZ-M",
        "outputId": "35af7a68-8c58-4e0d-d026-4e49d056fefe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "----------- smtk: 0 ------------\n",
            "Epoch 0/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not equal_num\n",
            "0\n",
            "[    1    21    34 ... 59952 59972 59987]\n",
            "5923\n",
            "1\n",
            "[    3     6     8 ... 59979 59984 59994]\n",
            "6742\n",
            "2\n",
            "[    5    16    25 ... 59983 59985 59991]\n",
            "5958\n",
            "3\n",
            "[    7    10    12 ... 59978 59980 59996]\n",
            "6131\n",
            "4\n",
            "[    2     9    20 ... 59943 59951 59975]\n",
            "5842\n",
            "5\n",
            "[    0    11    35 ... 59968 59993 59997]\n",
            "5421\n",
            "6\n",
            "[   13    18    32 ... 59982 59986 59998]\n",
            "5918\n",
            "7\n",
            "[   15    29    38 ... 59963 59977 59988]\n",
            "6265\n",
            "8\n",
            "[   17    31    41 ... 59989 59995 59999]\n",
            "5851\n",
            "9\n",
            "[    4    19    22 ... 59973 59990 59992]\n",
            "5949\n",
            "Selecting with ratios [0.09871667 0.11236667 0.0993     0.10218333 0.09736667 0.09035\n",
            " 0.09863333 0.10441667 0.09751667 0.09915   ]\n",
            "Class proportions [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-56e845b0a171>:488: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  order_mg_all = np.array(order_mg_all)\n",
            "<ipython-input-3-56e845b0a171>:489: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  cluster_sizes_all = np.array(cluster_sizes_all)\n",
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval time on training: 2.4894516468048096\n",
            "run: 0, grd_normw, subset_size: 0.4, epoch: 0, test_acc: 0.8925999999046326, loss: 0.4505839302539825, best_prec1_gb: 0.8925999999046326, not selected %:59.99166666666667\n",
            "Epoch 1/9\n",
            "not equal_num\n",
            "0\n",
            "[    1    21    34 ... 56952 56972 56987]\n",
            "5569\n",
            "1\n",
            "[    3     6     8 ... 56979 56984 56994]\n",
            "6288\n",
            "2\n",
            "[    5    16    25 ... 56983 56985 56991]\n",
            "5646\n",
            "3\n",
            "[    7    10    12 ... 56978 56980 56996]\n",
            "5817\n",
            "4\n",
            "[    2     9    20 ... 56943 56951 56975]\n",
            "5594\n",
            "5\n",
            "[    0    11    35 ... 56968 56993 56997]\n",
            "5239\n",
            "6\n",
            "[   13    18    32 ... 56982 56986 56998]\n",
            "5576\n",
            "7\n",
            "[   15    29    38 ... 56963 56977 56988]\n",
            "5924\n",
            "8\n",
            "[   17    31    41 ... 56989 56995 56999]\n",
            "5596\n",
            "9\n",
            "[    4    19    22 ... 56973 56990 56992]\n",
            "5751\n",
            "Selecting with ratios [0.09770175 0.11031579 0.09905263 0.10205263 0.09814035 0.09191228\n",
            " 0.09782456 0.10392982 0.09817544 0.10089474]\n",
            "Class proportions [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-56e845b0a171>:488: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  order_mg_all = np.array(order_mg_all)\n",
            "<ipython-input-3-56e845b0a171>:489: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  cluster_sizes_all = np.array(cluster_sizes_all)\n",
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval time on training: 2.355438232421875\n",
            "run: 0, grd_normw, subset_size: 0.4, epoch: 1, test_acc: 0.8950999975204468, loss: 0.41656049774822435, best_prec1_gb: 0.8950999975204468, not selected %:48.00333333333333\n",
            "Epoch 2/9\n",
            "not equal_num\n",
            "0\n",
            "[    1    21    34 ... 54102 54122 54137]\n",
            "5212\n",
            "1\n",
            "[    3     6     8 ... 54129 54134 54144]\n",
            "5819\n",
            "2\n",
            "[    5    16    25 ... 54133 54135 54141]\n",
            "5350\n",
            "3\n",
            "[    7    10    12 ... 54128 54130 54146]\n",
            "5537\n",
            "4\n",
            "[    2     9    20 ... 54093 54101 54125]\n",
            "5387\n",
            "5\n",
            "[    0    11    35 ... 54118 54143 54147]\n",
            "5074\n",
            "6\n",
            "[   13    18    32 ... 54132 54136 54148]\n",
            "5231\n",
            "7\n",
            "[   15    29    38 ... 54113 54127 54138]\n",
            "5600\n",
            "8\n",
            "[   17    31    41 ... 54139 54145 54149]\n",
            "5366\n",
            "9\n",
            "[    4    19    22 ... 54123 54140 54142]\n",
            "5574\n",
            "Selecting with ratios [0.09625115 0.10746076 0.09879963 0.102253   0.09948292 0.09370268\n",
            " 0.09660203 0.10341644 0.09909511 0.10293629]\n",
            "Class proportions [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-56e845b0a171>:488: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  order_mg_all = np.array(order_mg_all)\n",
            "<ipython-input-3-56e845b0a171>:489: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  cluster_sizes_all = np.array(cluster_sizes_all)\n",
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval time on training: 2.2383105754852295\n",
            "run: 0, grd_normw, subset_size: 0.4, epoch: 2, test_acc: 0.9065999984741211, loss: 0.37805235589777886, best_prec1_gb: 0.9065999984741211, not selected %:41.135\n",
            "Epoch 3/9\n",
            "not equal_num\n",
            "0\n",
            "[    1    21    34 ... 51396 51416 51431]\n",
            "4878\n",
            "1\n",
            "[    3     6     8 ... 51423 51428 51438]\n",
            "5384\n",
            "2\n",
            "[    5    16    25 ... 51427 51429 51435]\n",
            "5053\n",
            "3\n",
            "[    7    10    12 ... 51422 51424 51440]\n",
            "5272\n",
            "4\n",
            "[    2     9    20 ... 51387 51395 51419]\n",
            "5180\n",
            "5\n",
            "[    0    11    35 ... 51412 51437 51441]\n",
            "4938\n",
            "6\n",
            "[   13    18    32 ... 51426 51430 51442]\n",
            "4900\n",
            "7\n",
            "[   15    29    38 ... 51407 51421 51432]\n",
            "5280\n",
            "8\n",
            "[   17    31    41 ... 51411 51433 51439]\n",
            "5152\n",
            "9\n",
            "[    4    19    22 ... 51417 51434 51436]\n",
            "5406\n",
            "Selecting with ratios [0.0948234  0.10465953 0.09822522 0.10248236 0.10069397 0.09598974\n",
            " 0.09525105 0.10263787 0.10014968 0.10508718]\n",
            "Class proportions [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-56e845b0a171>:488: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  order_mg_all = np.array(order_mg_all)\n",
            "<ipython-input-3-56e845b0a171>:489: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  cluster_sizes_all = np.array(cluster_sizes_all)\n",
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval time on training: 2.130307674407959\n",
            "run: 0, grd_normw, subset_size: 0.4, epoch: 3, test_acc: 0.9096999764442444, loss: 0.3607933155107078, best_prec1_gb: 0.9096999764442444, not selected %:36.26833333333334\n",
            "Epoch 4/9\n",
            "not equal_num\n",
            "0\n",
            "[    1    21    34 ... 48826 48846 48861]\n",
            "4562\n",
            "1\n",
            "[    3     6     8 ... 48853 48858 48868]\n",
            "4962\n",
            "2\n",
            "[    5    16    25 ... 48857 48859 48865]\n",
            "4813\n",
            "3\n",
            "[    7    10    12 ... 48852 48854 48870]\n",
            "5010\n",
            "4\n",
            "[    2     9    20 ... 48817 48825 48849]\n",
            "4973\n",
            "5\n",
            "[    0    11    35 ... 48834 48842 48867]\n",
            "4789\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9abbbf30e1de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_logits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 indices, W_subset, _, _, ordering_time, similarity_time = get_orders_and_weights(\n\u001b[0m\u001b[1;32m    118\u001b[0m                     int(subset_size * len(X_train)), features, 'euclidean', smtk, 0, False, Y_train_nocat[keep_indices])\n\u001b[1;32m    119\u001b[0m                 \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeep_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-56e845b0a171>\u001b[0m in \u001b[0;36mget_orders_and_weights\u001b[0;34m(B, X, metric, smtk, no, stoch_greedy, y, weights, equal_num, outdir)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;31m#     lambda c: faciliy_location_order(c, X, y, metric, num_per_class[c], smtk, stoch_greedy, weights), classes))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;31m# pool.terminate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     order_mg_all, cluster_sizes_all, greedy_times, similarity_times = zip(*map(\n\u001b[0m\u001b[1;32m    476\u001b[0m         lambda c: faciliy_location_order(c, X, y, metric, num_per_class[c], smtk, no, stoch_greedy, weights), classes))\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-56e845b0a171>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;31m# pool.terminate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     order_mg_all, cluster_sizes_all, greedy_times, similarity_times = zip(*map(\n\u001b[0;32m--> 476\u001b[0;31m         lambda c: faciliy_location_order(c, X, y, metric, num_per_class[c], smtk, no, stoch_greedy, weights), classes))\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0morder_mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-56e845b0a171>\u001b[0m in \u001b[0;36mfaciliy_location_order\u001b[0;34m(c, X, y, metric, num_per_class, smtk, no, stoch_greedy, weights)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     order, cluster_sz, greedy_time, F_val = get_facility_location_submodular_order(\n\u001b[0m\u001b[1;32m    289\u001b[0m         S, num_per_class, c, smtk, no, stoch_greedy, weights)\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclass_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-56e845b0a171>\u001b[0m in \u001b[0;36mget_facility_location_submodular_order\u001b[0;34m(S, B, c, smtk, no, stoch_greedy, weights)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFacilityLocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_greedy_heap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mgreedy_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mF_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-f1c6af590132>\u001b[0m in \u001b[0;36mlazy_greedy_heap\u001b[0;34m(F, V, B)\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                     \u001b[0m_heappush_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimprov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0m_heappush_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-f1c6af590132>\u001b[0m in \u001b[0;36m_heappush_max\u001b[0;34m(heap, item)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_heappush_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mheap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mheapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_siftdown_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/heapq.py\u001b[0m in \u001b[0;36m_siftdown_max\u001b[0;34m(heap, startpos, pos)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;31m# newitem fits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstartpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mparentpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparentpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnewitem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(f'{folder}_{subset_size}_{grd}_{runs}_fcraig.npz')\n",
        "for key, value in data.items():\n",
        "    np.savetxt(\"/content/f_craig_10_100/\" + key + \".txt\", value)"
      ],
      "metadata": {
        "id": "bDPpnEq6ucrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/f_craig4.zip /content/f_craig_10_100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWT0etKElN7o",
        "outputId": "ce596bb1-8173-4d23-827f-f9f37ac36d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/f_craig_10_100/ (stored 0%)\n",
            "  adding: content/f_craig_10_100/fgt_score_time.txt (deflated 54%)\n",
            "  adding: content/f_craig_10_100/gow_time.txt (deflated 53%)\n",
            "  adding: content/f_craig_10_100/test_acc.txt (deflated 60%)\n",
            "  adding: content/f_craig_10_100/craig_time.txt (deflated 53%)\n",
            "  adding: content/f_craig_10_100/sim_time.txt (deflated 55%)\n",
            "  adding: content/f_craig_10_100/train_time.txt (deflated 57%)\n",
            "  adding: content/f_craig_10_100/test_loss.txt (deflated 55%)\n",
            "  adding: content/f_craig_10_100/times_selected.txt (deflated 97%)\n",
            "  adding: content/f_craig_10_100/epoch_time.txt (deflated 53%)\n",
            "  adding: content/f_craig_10_100/train_acc.txt (deflated 56%)\n",
            "  adding: content/f_craig_10_100/pred_time.txt (deflated 54%)\n",
            "  adding: content/f_craig_10_100/not_selected.txt (deflated 73%)\n",
            "  adding: content/f_craig_10_100/grd_time.txt (deflated 53%)\n",
            "  adding: content/f_craig_10_100/train_loss.txt (deflated 55%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic CRAIG"
      ],
      "metadata": {
        "id": "v6h9MupIz41E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.regularizers import l2\n",
        "#import utilities\n",
        "import sklearn.metrics\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "total_start = time.time()\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "\n",
        "num_classes, smtk = 10, 0\n",
        "Y_train_nocat = Y_train\n",
        "Y_train = np_utils.to_categorical(Y_train, num_classes)\n",
        "Y_test = np_utils.to_categorical(Y_test, num_classes)\n",
        "\n",
        "batch_size = 32\n",
        "# subset, random = False, False  # all\n",
        "subset, random = True, False  # greedy\n",
        "# subset, random = True, True  # random\n",
        "subset_size = .4 if subset else 1.0\n",
        "epochs = 10\n",
        "reg = 1e-4\n",
        "runs = 3\n",
        "save_subset = False\n",
        "\n",
        "folder = f'/content/'\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_dim=784, kernel_regularizer=l2(reg)))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(10, kernel_regularizer=l2(reg)))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='sgd')\n",
        "\n",
        "\n",
        "train_loss, test_loss = np.zeros((runs, epochs)), np.zeros((runs, epochs))\n",
        "train_acc, test_acc = np.zeros((runs, epochs)), np.zeros((runs, epochs))\n",
        "train_time = np.zeros((runs, epochs))\n",
        "epoch_time, craig_time = np.zeros((runs, epochs)), np.zeros((runs, epochs))\n",
        "grd_time, sim_time, pred_time, gow_time = np.zeros((runs, epochs)), np.zeros((runs, epochs)), np.zeros((runs, epochs)), np.zeros((runs, epochs))\n",
        "not_selected = np.zeros((runs, epochs))\n",
        "times_selected = np.zeros((runs, len(X_train)))\n",
        "best_acc = 0\n",
        "print(f'----------- smtk: {smtk} ------------')\n",
        "\n",
        "if save_subset:\n",
        "    B = int(subset_size * len(X_train))\n",
        "    selected_ndx = np.zeros((runs, epochs, B))\n",
        "    selected_wgt = np.zeros((runs, epochs, B))\n",
        "\n",
        "for run in range(runs):\n",
        "    X_subset = X_train\n",
        "    Y_subset = Y_train\n",
        "    W_subset = np.ones(len(X_subset))\n",
        "    ordering_time,similarity_time, pre_time = 0, 0, 0\n",
        "    loss_vec, acc_vec, time_vec = [], [], []\n",
        "    for epoch in range(0, epochs):\n",
        "        start_e = time.time()\n",
        "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
        "        num_batches = int(np.ceil(X_subset.shape[0] / float(batch_size)))\n",
        "\n",
        "        for index in range(num_batches):\n",
        "            X_batch = X_subset[index * batch_size:(index + 1) * batch_size]\n",
        "            Y_batch = Y_subset[index * batch_size:(index + 1) * batch_size]\n",
        "            W_batch = W_subset[index * batch_size:(index + 1) * batch_size]\n",
        "\n",
        "            start = time.time()\n",
        "            history = model.train_on_batch(X_batch, Y_batch, sample_weight=W_batch)\n",
        "            train_time[run][epoch] += time.time() - start\n",
        "        start_c = time.time()\n",
        "        if subset:\n",
        "            if random:\n",
        "                # indices = np.random.randint(0, len(X_train), int(subset_size * len(X_train)))\n",
        "                indices = np.arange(0, len(X_train))\n",
        "                np.random.shuffle(indices)\n",
        "                indices = indices[:int(subset_size * len(X_train))]\n",
        "                W_subset = np.ones(len(indices))\n",
        "            else:\n",
        "                start = time.time()\n",
        "                _logits = model.predict(X_train)\n",
        "                pre_time = time.time() - start\n",
        "                features = _logits - Y_train\n",
        "                gow_start = time.time()\n",
        "                indices, W_subset, _, _, ordering_time, similarity_time = get_orders_and_weights(\n",
        "                    int(subset_size * len(X_train)), features, 'euclidean', smtk, 0, False, Y_train_nocat)\n",
        "                go_time = time.time() - gow_start\n",
        "\n",
        "                W_subset = W_subset / np.sum(W_subset) * len(W_subset)  # todo\n",
        "\n",
        "            if save_subset:\n",
        "                selected_ndx[run, epoch], selected_wgt[run, epoch] = indices, W_subset\n",
        "\n",
        "            grd_time[run, epoch], sim_time[run, epoch], pred_time[run, epoch], gow_time[run, epoch] = ordering_time, similarity_time, pre_time, go_time\n",
        "            times_selected[run][indices] += 1\n",
        "            not_selected[run, epoch] = np.sum(times_selected[run] == 0) / len(times_selected[run]) * 100\n",
        "        else:\n",
        "            pred_time = 0\n",
        "            indices = np.arange(len(X_train))\n",
        "            \n",
        "        craig_time[run][epoch] = time.time() - start_c\n",
        "        X_subset = X_train[indices, :]\n",
        "        Y_subset = Y_train[indices]\n",
        "\n",
        "        start = time.time()\n",
        "        score = model.evaluate(X_test, Y_test, verbose=1)\n",
        "        eval_time = time.time()-start\n",
        "\n",
        "        start = time.time()\n",
        "        score_loss = model.evaluate(X_train, Y_train, verbose=1)\n",
        "        print(f'eval time on training: {time.time()-start}')\n",
        "\n",
        "        test_loss[run][epoch], test_acc[run][epoch] = score[0], score[1]\n",
        "        train_loss[run][epoch], train_acc[run][epoch] = score_loss[0], score_loss[1]\n",
        "        best_acc = max(test_acc[run][epoch], best_acc)\n",
        "\n",
        "        grd = 'random_wor' if random else 'grd_normw'\n",
        "        print(f'run: {run}, {grd}, subset_size: {subset_size}, epoch: {epoch}, test_acc: {test_acc[run][epoch]}, '\n",
        "              f'loss: {train_loss[run][epoch]}, best_prec1_gb: {best_acc}, not selected %:{not_selected[run][epoch]}')\n",
        "        epoch_time[run][epoch] = time.time() - start_e\n",
        "\n",
        "    if save_subset:\n",
        "        print(\n",
        "            f'Saving the results to {folder}_{subset_size}_{grd}_{runs}')\n",
        "\n",
        "        np.savez(f'{folder}_{subset_size}_{grd}_{runs}_craig',\n",
        "                 # f'_{grd}_{args.lr_schedule}_start_{args.start_subset}_lag_{args.lag}_subset',\n",
        "                 train_loss=train_loss, test_acc=test_acc, train_acc=train_acc, test_loss=test_loss,\n",
        "                 train_time=train_time, grd_time=grd_time, sim_time=sim_time, pred_time=pred_time,\n",
        "                 not_selected=not_selected, times_selected=times_selected,\n",
        "                 subset=selected_ndx, weights=selected_wgt,\n",
        "                 gow_time = gow_time,\n",
        "                 epoch_time=epoch_time, craig_time=craig_time)\n",
        "    else:\n",
        "        print(\n",
        "            f'Saving the results to {folder}_{subset_size}_{grd}_{runs}')\n",
        "\n",
        "        np.savez(f'{folder}_{subset_size}_{grd}_{runs}_craig',\n",
        "                 # f'_{grd}_{args.lr_schedule}_start_{args.start_subset}_lag_{args.lag}',\n",
        "                 train_loss=train_loss, test_acc=test_acc, train_acc=train_acc, test_loss=test_loss,\n",
        "                 train_time=train_time, grd_time=grd_time, sim_time=sim_time, pred_time=pred_time,\n",
        "                 not_selected=not_selected, times_selected=times_selected,\n",
        "                 gow_time = gow_time,\n",
        "                 epoch_time=epoch_time, craig_time=craig_time)\n",
        "total_time = time.time() - total_start\n",
        "print(total_time)"
      ],
      "metadata": {
        "id": "DTL9ggbp5EdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss2,test_acc2,train_acc2,test_loss2,train_time2,grd_time2,sim_time2,pred_time2,gow_time2, total_time2, epoch_time2,craig_time2=train_loss,test_acc,train_acc,test_loss,train_time,grd_time,sim_time,pred_time, gow_time, total_time, epoch_time,craig_time"
      ],
      "metadata": {
        "id": "_OS27g2s_O9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(f'{folder}_{subset_size}_{grd}_{runs}_craig.npz')\n",
        "for key, value in data.items():\n",
        "    np.savetxt(\"/content/craig1/\" + key + \".txt\", value)"
      ],
      "metadata": {
        "id": "x5DjCwmyoWfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/craig1.zip /content/craig1"
      ],
      "metadata": {
        "id": "2O_4l5FsoXJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}